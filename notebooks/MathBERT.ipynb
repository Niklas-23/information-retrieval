{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# S-Bert Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Union, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from arqmath_code.Entities.Post import Question, Answer\n",
    "from arqmath_code.topic_file_reader import Topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Simple S-Bert Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading users\n",
      "reading comments\n",
      "reading votes\n",
      "reading post links\n",
      "reading posts\n"
     ]
    }
   ],
   "source": [
    "from src import init_data\n",
    "topic_reader, data_reader = init_data(task=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at tbs17/MathBERT were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12])\n",
      "[CLS] replace me by any text you'd like. [SEP]\n"
     ]
    },
    {
     "data": {
      "text/plain": "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-2.2231, -1.3936,  0.5216,  ..., -1.4718, -0.6842,  0.3158],\n         [-0.2880, -1.4961,  1.6760,  ..., -0.4412, -1.4036,  0.4086],\n         [-0.4111, -1.3079,  0.4837,  ..., -2.2450, -1.1092,  0.1752],\n         ...,\n         [-1.8459, -2.0335, -0.6122,  ..., -0.8721, -0.5455, -0.2235],\n         [-0.1422,  1.6059, -0.3686,  ..., -0.8420, -0.2116, -0.2278],\n         [-0.6735,  1.3250,  0.5233,  ..., -0.5570,  0.3374,  0.9640]]],\n       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-1.1742e-01, -6.5391e-01,  8.7016e-01, -5.1339e-01,  1.7087e-01,\n         -5.5788e-01,  4.2671e-01,  6.1906e-01,  2.7943e-01, -3.5963e-01,\n          5.8281e-01, -4.7622e-01,  1.3670e-01,  1.6635e-01, -5.8218e-01,\n          1.1004e-01, -3.2577e-01, -4.3149e-01,  6.6923e-01, -2.8001e-01,\n          8.2339e-01, -7.1742e-01,  4.8106e-01,  3.1566e-01, -6.8671e-01,\n          5.8721e-01,  7.0809e-01, -4.5651e-01, -7.0852e-01,  6.4949e-01,\n          8.6123e-01,  5.4421e-01, -3.5311e-01, -4.7649e-01,  3.8607e-01,\n          7.2102e-02, -5.4948e-01, -7.5266e-01, -3.5022e-01, -3.7560e-01,\n          1.5278e-01, -7.6659e-01, -7.6906e-01,  2.2988e-01,  3.2620e-01,\n          1.9773e-01, -7.1930e-01,  1.2900e-01,  7.0263e-02,  4.6524e-01,\n          2.5950e-01,  1.9293e-01, -1.0992e-01, -3.0565e-01,  2.5247e-01,\n          4.6686e-01,  2.5192e-01,  7.6550e-01, -3.1103e-01, -1.8034e-01,\n          1.2689e-01, -3.2014e-01, -5.2186e-01,  2.1497e-02, -5.4645e-01,\n          4.2303e-01, -6.3684e-01, -7.5027e-01, -3.0005e-01,  4.1604e-01,\n         -5.8342e-02, -4.0347e-01,  2.0197e-01,  1.0306e-01,  6.6835e-01,\n          6.6945e-01,  2.6728e-01,  4.6431e-01, -3.9648e-01, -7.8188e-01,\n         -6.3983e-01,  4.8805e-01,  4.3357e-01,  6.0108e-01,  2.1380e-02,\n         -5.3501e-01, -6.2992e-01, -1.8008e-01, -9.3110e-01,  1.4191e-01,\n         -3.1409e-01,  5.8675e-01,  7.1820e-01, -1.5920e-01,  4.1632e-01,\n         -4.5425e-02, -5.5014e-01, -6.5010e-01, -1.4803e-01, -4.6288e-01,\n         -6.4420e-01, -9.9460e-02,  5.7941e-01, -2.1030e-01, -3.5960e-01,\n         -5.0499e-01, -8.4918e-02,  3.2477e-01, -1.5182e-01,  6.8165e-01,\n          5.6457e-01, -2.9875e-01,  4.3739e-01,  5.7538e-01,  5.9571e-02,\n          4.9329e-01,  4.9453e-01, -7.6673e-01,  5.5172e-01,  4.3312e-01,\n          7.8122e-01,  3.6174e-01,  5.1036e-01, -4.5944e-01,  7.3150e-01,\n         -4.3238e-01,  6.6637e-01,  1.5493e-01, -3.9903e-01, -7.3706e-01,\n         -7.2965e-01, -6.6974e-02, -4.5161e-01,  1.2444e-01, -7.3705e-01,\n         -1.3067e-01, -8.2116e-01,  1.5504e-01, -5.2217e-01,  4.9593e-01,\n         -8.5896e-01,  4.0634e-01, -8.4951e-01,  2.6913e-01, -6.9675e-01,\n          5.9826e-01, -6.1092e-02,  8.0749e-02, -4.4315e-01,  1.8334e-01,\n         -4.0289e-02, -4.6995e-01, -1.9567e-01, -4.1615e-01, -3.7436e-01,\n         -7.7290e-01,  5.5569e-01,  3.3730e-01, -8.0553e-01, -5.9330e-01,\n         -3.7856e-01,  7.7382e-01, -1.6434e-01,  2.2900e-01, -3.9452e-01,\n          5.3760e-01, -8.5110e-01,  4.5752e-01, -8.5116e-01, -2.5488e-01,\n          1.0688e-01,  2.8140e-01,  7.8040e-01, -3.5739e-01, -2.3296e-01,\n          6.4363e-01, -5.8635e-01, -2.5359e-01,  8.5160e-01, -2.3505e-01,\n         -6.4910e-01,  1.1227e-01, -4.3212e-01, -5.4609e-01, -3.3045e-01,\n         -8.5060e-01,  3.6947e-01, -5.2966e-01, -2.6077e-01,  7.2681e-01,\n          7.8141e-01,  6.1522e-01,  5.8932e-01,  4.3580e-01, -5.9810e-01,\n          1.6999e-02, -2.4976e-01, -5.1272e-01,  8.6005e-01,  4.8050e-01,\n          1.6674e-01,  8.8877e-01,  1.5450e-01, -1.2447e-01,  6.8840e-01,\n          5.0219e-01,  4.3709e-02,  5.8268e-01, -4.6740e-01, -8.1051e-01,\n          3.8452e-01, -4.8421e-01, -5.3308e-01, -3.3758e-01,  4.9875e-01,\n          2.6917e-01, -4.8117e-01,  4.0514e-01,  6.1053e-01,  8.3559e-02,\n          5.8201e-01,  5.3901e-01, -5.4129e-01,  5.5081e-01, -2.5566e-01,\n         -5.3667e-04, -3.2493e-01,  8.7215e-01,  3.4829e-01, -6.7410e-01,\n         -4.3068e-01,  3.3837e-01, -7.1121e-01, -5.9826e-01,  1.3175e-01,\n          3.2510e-02,  4.4879e-01,  5.0422e-02, -6.0983e-01, -4.4137e-01,\n          1.6736e-01,  3.7941e-03, -5.3048e-01, -9.8935e-02,  4.0112e-01,\n          4.3717e-01,  9.2910e-01, -3.4674e-01, -6.8598e-01,  6.9531e-01,\n          3.6474e-01, -6.5753e-01, -6.1176e-01, -5.4787e-01, -2.2992e-01,\n          6.2861e-01,  7.9696e-01,  4.2325e-01, -1.2650e-01, -2.6475e-01,\n         -5.2121e-01, -5.1585e-01, -7.6321e-01,  1.8331e-01, -1.5954e-02,\n          5.7735e-01, -6.0407e-01,  6.5406e-01,  1.2924e-01, -6.4920e-01,\n         -7.0291e-01,  7.6138e-01, -7.5609e-01, -3.7832e-01,  2.0745e-01,\n         -2.3276e-01,  3.9965e-01, -6.1053e-01, -4.3448e-01,  2.4787e-01,\n         -7.4011e-01, -7.4413e-01, -6.3664e-01, -4.6449e-01, -7.5181e-02,\n          7.7982e-01, -1.2551e-01, -7.5537e-01,  6.8542e-01,  4.1871e-01,\n          2.9233e-01,  7.7246e-01, -3.3714e-01, -1.0859e-01, -8.2538e-01,\n          3.3538e-01, -3.2774e-01, -8.0309e-01,  2.0130e-01, -3.2226e-01,\n         -7.4356e-01,  4.0629e-01,  1.8200e-02, -2.5160e-02,  1.8043e-01,\n          1.7636e-01,  4.8969e-01,  4.2275e-01, -7.8235e-01,  1.5116e-01,\n         -4.0743e-01,  5.2978e-02,  7.3975e-01, -4.9829e-01,  7.7729e-01,\n          1.0666e-01, -6.6804e-01, -4.2688e-01, -1.8307e-01,  8.0246e-01,\n         -6.3212e-01,  4.9381e-02, -1.9473e-01,  7.2717e-01, -2.7067e-01,\n          8.0965e-01,  1.0140e-01, -9.0948e-01, -8.8065e-01, -3.1291e-02,\n          2.3936e-01,  4.7864e-01,  7.3359e-01, -2.0580e-01, -4.8849e-01,\n          1.3751e-01, -6.5959e-01,  3.6963e-01,  6.5230e-01,  5.9989e-01,\n          6.0427e-01, -6.1691e-01,  7.5307e-01, -4.0507e-01,  4.4554e-01,\n         -1.5215e-01,  4.7475e-01,  7.2617e-01, -8.3028e-01,  6.7467e-01,\n         -5.7507e-01, -1.0149e-03,  3.0514e-01,  8.6351e-01, -5.5635e-01,\n         -7.9572e-02, -7.1804e-01, -5.3014e-01, -3.9346e-01, -7.3529e-01,\n         -6.7240e-01,  3.9788e-01,  1.0880e-01, -9.0467e-02, -9.6490e-02,\n          5.8603e-01, -4.2884e-01, -4.1176e-01, -7.8670e-01, -1.4995e-01,\n         -4.1737e-01, -7.6236e-01, -7.1463e-01,  4.5331e-01,  2.3107e-01,\n         -8.9468e-01, -5.7267e-01,  7.4580e-01, -4.7294e-01,  1.1931e-01,\n         -1.1622e-01,  5.1330e-01,  4.4987e-01,  7.5506e-01, -2.9554e-01,\n          1.7591e-01, -2.6481e-01,  4.5828e-01,  5.8560e-01, -5.2734e-01,\n          7.9028e-01,  3.4921e-01,  3.0845e-02,  6.1738e-01,  6.7164e-01,\n          2.1094e-01,  1.4402e-01, -2.3696e-01,  4.7621e-01,  6.5978e-01,\n         -1.2963e-01, -6.2278e-01,  4.8947e-01, -7.3006e-01, -6.1780e-01,\n         -6.0771e-01, -4.1518e-01,  1.0071e-01,  3.7899e-02,  6.1413e-01,\n         -7.1912e-01, -8.1752e-01,  5.7227e-01, -6.9923e-01,  8.3469e-01,\n          7.0466e-02,  4.3993e-01,  4.7230e-01, -4.4570e-01, -7.6817e-03,\n         -1.3398e-01, -6.4329e-02, -5.2731e-01, -8.0069e-01, -5.8153e-01,\n          3.1852e-01,  3.1334e-01,  2.7307e-02, -3.0151e-01,  1.1047e-02,\n          1.5852e-01, -6.2901e-01, -3.7259e-01,  4.0798e-01,  3.1007e-01,\n         -1.9901e-01,  3.0818e-01, -1.2939e-01, -7.8322e-01,  7.4262e-01,\n         -2.1759e-01, -4.5975e-01, -6.6459e-01,  5.7186e-03,  6.7219e-01,\n          7.8288e-01, -4.5396e-01, -7.3452e-01, -5.2063e-01, -4.1839e-02,\n         -6.4056e-02, -1.0892e-02,  4.5141e-01,  1.6868e-01,  1.1599e-01,\n          6.0612e-01, -9.0558e-01,  4.3275e-01,  4.3373e-02,  6.8065e-01,\n          2.3144e-01,  4.7494e-01, -6.7725e-01,  1.1008e-01, -5.7973e-01,\n          2.6840e-01, -2.2708e-01, -1.5481e-02, -7.6014e-01,  4.1769e-01,\n          4.0136e-01, -3.8190e-01, -7.0323e-01, -7.4253e-01,  3.9099e-02,\n          1.4667e-01, -2.7573e-01,  4.2066e-01,  6.0242e-01,  1.7394e-01,\n         -5.4412e-01, -7.5588e-01, -6.1153e-01,  7.8172e-01,  2.4041e-01,\n          1.5085e-01, -6.1612e-01,  3.2398e-01, -7.8389e-01,  7.5710e-01,\n         -5.9217e-01,  7.7312e-02, -5.0776e-01, -3.6813e-02, -7.1931e-01,\n         -6.8953e-01, -5.6829e-01, -3.3664e-02, -6.5460e-01, -2.7827e-01,\n         -8.2370e-02,  7.3122e-01, -7.0940e-01, -2.1276e-01,  2.2833e-01,\n         -9.0940e-02,  1.2908e-01,  4.0335e-02,  4.6498e-01,  7.2158e-01,\n         -5.8117e-02,  4.9709e-02,  1.7598e-01, -6.6589e-01,  5.1241e-01,\n          5.0867e-02, -9.5140e-01, -4.0994e-01, -2.4066e-01, -1.5974e-01,\n          2.9646e-01, -2.5470e-01, -7.4232e-01, -5.3969e-01, -4.0987e-01,\n         -1.9916e-01,  4.3437e-01, -4.0543e-01, -3.7250e-01,  7.1622e-01,\n          7.4446e-01, -8.3245e-01,  4.1110e-01, -5.1806e-01,  7.2865e-01,\n         -6.1584e-01, -7.7046e-01, -7.1984e-01, -1.6027e-01,  5.8130e-03,\n         -3.1517e-01,  6.4504e-01,  5.4601e-02, -1.8478e-01, -3.1127e-01,\n          7.6647e-01,  5.7621e-01,  8.7692e-01,  3.3937e-01, -5.4913e-01,\n          2.8983e-01,  7.0901e-01,  3.8205e-02,  5.9187e-01,  9.2823e-01,\n         -6.5104e-01,  5.3417e-01, -4.7314e-01,  7.4708e-01,  4.1115e-01,\n          3.5397e-01, -1.8049e-01, -5.9355e-01,  4.0703e-01,  4.9079e-01,\n         -3.2908e-01, -7.7924e-01, -4.9142e-01,  2.3423e-01,  4.3926e-01,\n         -3.7114e-02, -6.4901e-01,  1.0183e-01,  1.5078e-01,  2.5443e-01,\n          4.3814e-02, -8.2339e-02, -3.8148e-01, -1.6082e-01, -3.5546e-01,\n         -4.6202e-01, -2.9262e-01,  4.3083e-01, -8.0843e-01,  5.6324e-01,\n          1.9628e-01, -9.0248e-01,  1.9199e-01,  6.6548e-01, -6.4614e-01,\n          5.3278e-01,  2.3704e-01, -7.9258e-01,  5.2365e-01,  2.0000e-01,\n         -5.4157e-01,  4.4061e-01, -5.6681e-01, -7.3732e-01,  6.6598e-01,\n         -2.6584e-01, -4.4132e-01, -7.7483e-01,  1.7266e-01,  1.7445e-01,\n         -5.4556e-01,  7.9814e-01, -7.1188e-01, -1.5453e-01, -3.7087e-01,\n          6.0251e-01, -8.2166e-01, -4.5029e-01, -9.2759e-02,  7.3235e-01,\n         -1.1168e-01, -2.0777e-01,  7.8614e-01, -7.3049e-01, -7.9636e-02,\n          1.1066e-01, -6.4836e-01, -5.7497e-01,  7.6441e-01, -5.5656e-01,\n         -7.4858e-01,  6.1088e-01,  9.8601e-02,  5.0824e-01, -4.9950e-01,\n         -3.7633e-01, -3.7226e-01,  8.0445e-01,  5.0040e-01,  6.8591e-01,\n          5.0630e-01,  4.8441e-01,  7.9696e-01, -3.3414e-01,  6.9567e-01,\n         -1.5953e-01, -8.0136e-02,  1.8431e-02,  5.4657e-01,  2.4742e-01,\n         -1.9748e-01,  8.2531e-01, -2.2320e-01,  3.4314e-02,  8.4774e-01,\n          1.7078e-01,  7.5753e-01,  7.9041e-01,  3.8807e-01, -3.1659e-02,\n          6.3638e-01, -8.8158e-01,  3.3890e-01,  2.8185e-01,  5.6239e-01,\n          2.4043e-01,  4.5651e-01, -5.2660e-01,  2.0315e-01,  4.9899e-01,\n          5.0956e-01, -5.9313e-01, -6.4754e-01,  6.8966e-01,  2.0247e-01,\n         -4.2045e-01, -3.0366e-02, -5.4445e-01,  2.6582e-01, -4.8026e-01,\n          8.5517e-03,  2.7642e-01,  2.0718e-01,  3.4969e-01, -6.9277e-01,\n          2.6644e-01, -7.7092e-01, -5.7953e-01,  5.1948e-01, -1.0096e-01,\n          8.6893e-02, -2.5551e-01, -2.8540e-01, -4.6774e-01,  6.7748e-01,\n         -3.0607e-01, -2.8982e-01,  4.2995e-01, -8.1146e-01,  4.8845e-01,\n          6.9520e-01,  5.6359e-01, -1.1790e-01,  1.3444e-01, -5.5870e-01,\n          2.4631e-01,  5.1561e-01, -2.0009e-01, -2.6704e-02, -7.9538e-01,\n          7.2827e-01, -7.0471e-01,  2.6667e-01,  8.5771e-01,  5.9249e-01,\n         -2.2855e-01,  9.2154e-02, -4.6043e-01, -2.1519e-01,  6.4581e-01,\n          1.9175e-01, -5.0286e-01,  7.3250e-01,  1.1671e-01,  7.1199e-02,\n          2.7037e-01, -2.6825e-01, -2.0981e-01,  7.9814e-02, -3.6284e-01,\n         -1.2653e-01,  5.8768e-01,  4.3649e-01,  5.9399e-01,  5.7595e-01,\n          5.5833e-01,  6.1141e-01, -8.2730e-01,  1.9584e-01,  3.6805e-01,\n         -6.4180e-01,  6.9902e-01,  5.7975e-01, -9.3405e-02, -1.5000e-01,\n          4.2039e-01,  6.4855e-01,  8.7404e-01,  6.4240e-01,  2.3187e-01,\n         -1.6501e-01,  1.8792e-01,  4.4387e-01,  3.1792e-01,  3.3557e-01,\n         -5.1620e-01,  4.7170e-02,  4.2981e-01, -2.4167e-01, -7.1741e-01,\n         -2.3833e-01,  2.8367e-01,  4.5617e-01,  1.7046e-01, -7.4517e-01,\n         -2.6187e-01,  8.8055e-01,  4.7721e-01,  7.5731e-01,  1.1386e-01,\n         -8.4558e-01,  1.4943e-01, -1.2968e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained(\"tbs17/MathBERT\", output_hidden_states=True)\n",
    "model = BertModel.from_pretrained(\"tbs17/MathBERT\")\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')[\"input_ids\"]\n",
    "print(encoded_input.size())\n",
    "print(tokenizer.decode(encoded_input[0]))\n",
    "output = model(encoded_input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([-2.2231e+00, -1.3936e+00,  5.2164e-01, -1.1962e+00, -1.1041e+00,\n        -1.0853e+00, -6.9400e-01, -9.0696e-01, -1.2116e+00, -3.6992e-01,\n        -4.5352e-01,  9.4478e-01, -2.9552e-01,  6.9703e-01, -4.0970e+00,\n        -1.5467e-01, -1.3264e+00,  1.5981e+00,  7.3125e-03,  1.6195e+00,\n        -9.8833e-01,  4.4628e-01,  5.2678e-01,  6.0169e-01, -3.9136e-02,\n         8.1383e-01,  3.4131e+00, -6.0480e-01,  7.8776e-01, -2.1514e-01,\n         5.3082e-01,  2.7428e-01,  6.9694e-01,  1.0118e+00,  6.3361e-01,\n        -1.2968e+00,  6.0122e-01, -4.9637e-01,  2.3598e+00, -1.0558e+00,\n         7.8685e-01, -2.2897e+00,  1.6963e+00, -1.4375e+00,  6.1504e-01,\n         2.6631e+00, -1.1293e+00, -6.4592e-01,  3.5112e-01, -7.0518e-01,\n         7.3810e-01,  9.4142e-01, -5.9272e-01, -3.4221e-01,  9.7941e-01,\n         1.0112e+00, -3.8004e-02, -9.6038e-01, -1.0530e+00, -1.4943e+00,\n         1.8094e+00,  9.8292e-01, -2.0841e+00, -9.8258e-02, -4.6969e-01,\n         7.1740e-01, -7.1648e-01, -1.7884e+00, -8.6375e-02,  2.6076e-01,\n        -6.1328e-01, -7.7242e-01,  4.0503e-03, -1.5181e-01,  1.7528e+00,\n         5.7417e-01, -1.1233e+00, -8.2406e-01, -2.0038e-01, -4.9981e-01,\n        -9.6133e-01, -8.2647e-01,  1.3944e-01,  2.0326e-01, -2.5045e-01,\n         3.9199e-01,  2.8066e-01,  6.5562e-01,  2.9246e-01,  4.0076e-01,\n         6.2194e-01,  7.1251e-01, -4.9473e-01, -3.4826e-01, -2.9683e-01,\n        -1.3276e+00, -5.0891e-01,  1.3743e+00, -1.3374e+00,  4.0816e-01,\n         1.2172e+00,  1.3407e+00,  1.7255e+00, -6.7442e-01,  7.9212e-01,\n         1.3090e+00,  9.3783e-01,  1.0224e-01,  1.4451e+00,  7.2818e-01,\n         7.7608e-01,  6.6802e-01,  1.1208e+00, -1.3031e+00,  1.1652e+00,\n         2.4436e-01,  4.9278e-01, -3.0911e-01, -9.1782e-01,  8.2528e-01,\n         4.9277e-01, -1.0819e+00,  7.4387e-01, -2.2627e+00,  1.0970e-01,\n         9.8199e-01, -1.4149e+00,  1.7286e-01, -1.0468e-01, -3.8767e-01,\n        -3.5042e-02,  6.3613e-02,  1.5895e-01, -1.0156e+00, -1.6421e+00,\n         1.6593e-01,  5.9414e-01,  6.9255e-01,  2.2645e+00,  1.1887e+00,\n         9.6738e-01, -4.5955e-01, -7.9203e-01, -3.5117e-01,  2.7933e-04,\n         1.3881e+00,  1.3377e+00,  2.2247e-01,  9.1865e-01, -1.8478e+00,\n        -2.6385e-01, -1.3607e+00,  2.0738e-01,  2.6468e-01,  5.6208e-01,\n        -9.5493e-01, -2.2765e-01,  5.2174e-01, -5.4934e-03, -1.0044e+00,\n         2.5824e-01, -4.1618e-01, -9.0783e-01, -3.8282e-02, -1.3078e+00,\n        -1.6368e-01,  1.9281e+00,  2.3044e+00, -3.6090e-01,  9.2771e-01,\n        -7.3141e-01,  1.1059e+00, -5.5282e-02,  8.4455e-01,  4.1584e-01,\n         2.5361e-01,  2.6202e-01,  1.4641e+00, -5.5081e-01, -5.0906e-01,\n         1.2431e-01,  3.8910e-01,  3.4822e-01,  3.3601e-01, -1.2990e+00,\n         1.1669e+00, -1.0671e-01, -8.5230e-01, -1.8179e+00,  2.5216e-01,\n        -4.2159e-01,  2.1409e+00,  8.9310e-01,  1.6407e+00, -1.1701e+00,\n        -1.2080e+00, -4.6952e-01, -4.4406e-02,  1.8343e+00,  3.1728e-01,\n        -1.1076e+00,  2.1347e-01,  1.2882e+00,  3.1367e-01, -1.2961e-01,\n         4.7842e-01,  5.1949e-01,  1.3522e-01,  1.0102e+00, -6.1838e-01,\n         5.6765e-01,  2.6925e-01, -2.9325e-01, -4.0473e-01,  5.1197e-02,\n         1.2868e+00, -1.0815e-01,  7.4658e-01, -3.1375e-01,  3.4264e-01,\n        -4.6937e-02,  1.3854e-01, -9.8283e-01, -1.2786e+00,  2.0229e+00,\n        -3.3814e-01,  7.1226e-01,  3.9642e-01,  1.6592e+00,  3.5908e-03,\n         4.1494e-01,  2.0795e-01, -1.2054e+00, -6.4023e-01, -3.3273e-01,\n         2.2898e+00, -3.7466e-01, -1.6679e+00, -9.7543e-01, -1.0513e+00,\n         8.5114e-02,  2.2707e+00, -1.1975e-01,  2.3198e-01,  1.1430e-01,\n        -1.1883e-01,  3.0257e-01,  4.7361e-01, -1.0024e+00, -5.6926e-01,\n        -8.0159e-01,  9.0018e-01,  4.4596e-01,  1.7220e-01,  8.3588e-02,\n         4.6652e-01,  1.1265e+00,  3.1356e-01,  8.5455e-01,  5.3571e-01,\n         8.2972e-01,  3.9879e-01,  7.5064e-01,  5.8239e-01, -6.5114e-01,\n         4.5557e-01, -6.0264e-01, -2.8541e-01,  1.3854e+00, -1.5744e+00,\n         6.5760e-01,  5.1764e-02,  7.0915e-01,  1.9077e-01, -1.0271e+00,\n         9.3838e-01,  2.9716e+00, -4.5884e-01, -1.0921e+00,  1.4613e+00,\n        -9.2740e-01, -1.0629e+00, -7.2912e-01, -3.1381e+00, -1.4079e+00,\n         2.5566e-01,  3.1734e-01,  8.9673e-01,  2.1888e-02,  9.0356e-01,\n        -1.2201e+00, -1.2061e+00,  3.0028e-01,  4.6006e-01,  6.0100e-01,\n        -1.2348e+00,  6.8718e-01, -6.1631e-01, -3.4310e-01,  1.5717e+00,\n         4.7255e-01,  1.1969e+00, -9.0987e-01,  2.1676e+00,  8.4793e-01,\n        -5.5083e-01, -1.2339e+00,  5.5533e-01,  6.9791e-01,  6.7838e-01,\n         4.7313e-01, -1.0766e+00,  7.6345e-01, -2.6323e-02, -1.3934e+00,\n         1.1498e+00,  5.1342e-01, -1.8471e+00,  5.3230e-01,  3.3204e-01,\n         1.2503e+00,  3.1248e-01, -1.7609e+00, -9.4465e-01,  1.2272e+00,\n         2.5525e+00, -8.5249e-01,  9.9212e-01, -8.2803e-01,  5.9036e-01,\n         1.9953e+00, -5.8372e-02,  4.9184e-02, -3.4784e-01, -4.5748e-01,\n         5.3293e-01, -7.1668e-01,  9.9257e-01,  1.4012e+00, -1.2153e+00,\n         7.4534e-01, -3.7429e-01,  5.0115e-01, -1.2168e+00,  9.1134e-01,\n         8.8844e-01, -4.5709e-01,  8.0498e-01,  2.1841e-01,  5.2628e-01,\n        -2.3053e-01, -1.3588e+00, -2.4428e-01, -3.4557e+00,  2.5739e+00,\n         7.3361e-01,  8.4630e-01, -1.3442e+00, -1.7232e+00,  8.9297e-01,\n        -1.0571e+00, -8.0477e-01,  1.1297e+00,  7.1882e-02,  2.7884e-01,\n         4.3562e-01,  8.6468e-01,  2.8219e-01, -6.6462e-01, -5.2251e-01,\n        -6.2415e-01,  8.2150e-01, -4.2226e-01, -6.8687e-03,  4.8653e-01,\n         1.6978e+00, -5.1171e-01, -3.3772e-01,  8.5321e-02, -9.3524e-01,\n        -7.8715e-01, -4.9218e-02,  3.7858e-01, -8.1782e-01,  1.3649e+00,\n        -3.5418e-02,  2.7059e-01,  1.7000e+00,  5.3495e-01,  4.7613e-02,\n        -2.0086e-01, -5.9553e-01,  7.7862e-01, -4.6893e-01,  2.4238e-02,\n        -1.7861e-01, -1.3714e-01, -8.5588e-01,  3.3006e-01,  2.2147e+00,\n         1.8013e-01,  6.3916e-01,  3.3231e-01, -1.4194e+00, -2.7147e-01,\n        -8.5677e-01, -7.6455e-01, -4.3116e-01, -9.9441e-01, -7.4077e-03,\n         9.8141e-01, -1.2998e+00, -1.4860e+00, -6.5360e-01,  1.0108e-01,\n         1.0971e+00, -1.5441e-01, -8.8550e-01,  2.4270e-01,  7.4194e-01,\n         1.4429e+00,  1.0147e-01,  1.0946e+00, -5.9066e-01,  7.3771e-02,\n         7.3922e-01, -1.1959e-01,  2.5994e+00,  1.2280e+00, -1.9110e-01,\n        -1.3612e+00, -3.6537e-01,  7.2820e-01,  5.1851e-01, -1.0253e-01,\n        -8.3084e-01,  1.6038e+00, -8.9329e-01, -2.8103e-01, -3.4099e-01,\n        -6.3078e-01, -1.3254e+00, -6.7980e-01, -4.0994e-01, -5.9859e-02,\n         3.8289e-01,  1.3280e-01,  9.8549e-01,  1.1675e-01,  2.0238e-01,\n        -2.0920e+00, -3.3281e-01, -1.1736e+00,  8.1255e-01, -1.5098e+00,\n         1.7278e-01, -2.9327e-01, -1.7317e+00,  1.4723e-01, -1.7690e+00,\n         1.1840e-02,  3.5850e-01,  5.9565e-01, -1.8306e+00,  2.8434e-01,\n        -6.4252e-01, -5.3641e-01, -4.8185e-01,  8.1159e-01,  2.4486e-01,\n        -2.5514e+00, -7.3551e-01,  2.5524e-01, -1.0051e+00, -3.5186e-01,\n        -3.3102e-01, -1.5016e+00,  2.2339e-01, -6.8770e-02,  7.9534e-01,\n         1.1195e+00, -5.2741e-01,  4.9267e-01,  8.5473e-01, -3.7831e-01,\n         1.3170e-01,  6.5933e-01, -4.4154e-02, -6.2571e-01,  5.9802e-02,\n        -4.7192e-01, -6.0888e-01, -3.1808e-01, -2.4655e-01,  4.8221e-01,\n        -1.8266e+00,  1.4390e+00,  3.2005e-01,  1.6276e+00,  1.6867e+00,\n         1.4099e-02,  9.8505e-01, -2.7140e-01, -2.0097e+00,  5.5365e-01,\n        -3.6011e-01,  8.7594e-01,  6.2656e-01, -1.0348e+00,  4.8669e-01,\n        -1.1662e+00, -3.7010e-01, -2.3775e+00,  1.9196e-01, -1.5859e+00,\n         1.9126e-01,  1.2041e+00, -3.1735e-01,  6.8127e-01, -3.3426e-01,\n         3.1011e-01,  9.5201e-01,  8.3464e-01, -1.7738e-02, -2.8071e-01,\n         6.3630e-01, -1.3403e+00, -4.3150e-01, -4.5323e-01,  3.7795e-01,\n        -6.8698e-01, -8.0873e-01, -9.5102e-01, -1.2934e+00, -1.0049e+00,\n        -1.1850e+00,  1.9207e-01,  4.5953e-01, -1.1546e+00, -1.2234e+00,\n         3.4246e-01, -7.0726e-01, -1.0779e-01, -6.0570e-01,  7.7023e-01,\n         1.5884e+00, -1.0677e+00, -7.7146e-01,  7.7565e-02,  1.1094e+00,\n        -7.8569e-01,  3.2539e-01,  1.9787e-01,  5.7095e-01,  1.5454e+00,\n        -1.0336e+00, -1.6024e+00, -1.3606e+00, -1.4431e+00,  1.2605e+00,\n        -2.7778e-01, -1.1175e+00, -1.2642e+00, -1.8096e-01, -9.9476e-01,\n        -2.4646e-02,  7.0267e-01, -1.8445e+00, -4.8553e-01, -7.8359e-01,\n        -1.6562e-01,  2.4047e-01,  2.8763e-02, -1.1468e+00,  7.4538e-01,\n        -6.4798e-01, -4.7403e-01, -7.2057e-01,  7.7043e-01, -4.1393e-02,\n         6.2383e-01,  1.6842e+00, -1.0189e+00, -1.1992e+00,  8.0429e-01,\n        -6.9808e-02,  5.0831e-01, -5.2663e-01,  4.7790e-01, -5.9457e-01,\n         1.8394e+00, -6.0222e-01, -3.4964e-01, -1.8229e+00,  1.0854e-01,\n         2.1048e+00,  6.8384e-01,  1.1901e+00,  6.1169e-01,  1.2541e+00,\n        -3.6732e-01,  3.0444e-01,  1.5886e+00, -9.2571e-01, -1.6289e+00,\n         2.1224e+00,  1.1733e+00, -1.4922e+00,  1.2127e+00, -2.6171e-01,\n         5.2740e-01, -2.9179e+00,  2.1098e+00, -4.2666e-02, -2.6214e-01,\n        -1.1151e+00, -3.3289e-01, -1.3155e+00,  2.0180e-01, -4.2997e-01,\n        -2.2346e-01, -2.5963e+00, -1.1213e+00,  1.6484e+00,  1.0044e+00,\n        -3.7528e-01, -1.5625e+00, -1.1395e+00, -1.9324e-01, -2.4091e-01,\n         2.2557e-01, -1.3312e+00,  1.1935e+00,  7.7774e-02,  4.4334e-01,\n         1.0489e+00,  8.2233e-01, -5.6374e-01,  7.2545e-01, -3.8329e-01,\n        -4.0503e-01, -1.0908e+00, -6.5857e-01,  1.6223e+00,  5.9130e-01,\n        -1.3189e-01, -7.2181e-01, -7.1615e-01,  6.8049e-01,  1.9565e-01,\n        -1.3859e-01, -1.7512e-01, -8.4024e-01, -1.6498e+00, -1.2379e+00,\n        -5.1304e-01, -3.3220e-01, -2.4349e-01, -2.2882e-01,  2.6751e-01,\n         1.0206e+00, -6.5364e-01,  1.0288e+00, -7.7360e-01, -9.7602e-01,\n        -2.0477e+00,  1.4468e+00,  4.6105e-01,  1.0671e+00, -2.4875e-02,\n         6.7150e-01, -4.3434e-01, -1.5040e+00, -2.1207e+00,  3.9274e-01,\n        -4.9097e-01, -6.5929e-01, -5.5474e-01, -5.9495e-01,  1.3943e+00,\n        -3.4666e-01, -8.2582e-01, -6.4971e-01,  4.0377e-01,  1.7821e-01,\n         4.8186e-01,  3.9348e-01,  6.2640e-01,  6.4229e-01, -1.0547e+00,\n        -3.4605e-01,  7.7377e-01,  1.7478e-01,  1.2405e+00, -1.0911e+00,\n         1.9967e-01, -3.7006e-01, -4.4870e-01,  6.9015e-01, -4.7883e-01,\n        -5.5197e-01,  1.7952e+00, -2.2729e-01, -5.1965e-01,  1.7709e+00,\n         8.9630e-01,  1.0809e+00,  3.7246e-01, -4.0693e-01, -7.5725e-01,\n        -2.9228e-01,  3.9373e-01,  3.5598e-01,  7.4744e-01, -9.2148e-02,\n         8.9127e-01,  1.4860e+00, -1.5695e-01, -7.4596e-02,  4.4429e-01,\n         1.1429e+00,  1.1391e+00,  7.1202e-02,  8.6735e-01,  7.8032e-01,\n         3.5113e-01,  7.8156e-01, -4.3247e-01, -9.3663e-01, -9.6863e-01,\n         1.6579e+00, -1.2550e-01, -8.6843e-01,  9.7184e-01,  1.4442e+00,\n         1.1261e+00,  2.0733e+00, -1.5252e+00, -1.0729e+00,  6.7500e-01,\n         9.4675e-01, -8.6430e-01, -1.4330e+00, -6.8903e-01, -2.2438e-01,\n        -1.5563e+00,  8.0643e-01,  9.9448e-01,  5.1110e-02,  4.5418e-01,\n         2.8689e-01, -6.2778e-01, -3.7393e-01,  1.3274e-01,  2.5397e+00,\n         1.0624e+00,  1.2038e+00, -1.0149e+00, -4.9809e-01, -1.6519e-01,\n         1.1462e-01,  1.2837e-01, -7.5223e-01, -2.2448e-01, -2.0609e+00,\n        -1.4718e+00, -6.8422e-01,  3.1579e-01], grad_fn=<SelectBackward0>)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(output.last_hidden_state.size())\n",
    "output.last_hidden_state[0,0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from src.mathbert.question_math_bert import QuestionMathBERT\n",
    "# from src.sbert.question_s_bert import QuestionSBERT\n",
    "from src.post_processors.top_k_filter import TopKFilter\n",
    "from arqmath_code.post_reader_record import DataReaderRecord\n",
    "from src.base.pipeline import Pipeline\n",
    "from src.post_processors.answer_score_retriever_for_questions import AnswerScoreRetrieverForQuestions\n",
    "\n",
    "class MathBertPipeline(Pipeline):\n",
    "\n",
    "    def __init__(self, data_reader: DataReaderRecord):\n",
    "        super().__init__(data_reader)\n",
    "        self.sbert = QuestionMathBERT('MathBERT')\n",
    "#         self.sbert = QuestionSBERT(model_id='tbs17/MathBERT')\n",
    "        self.answer_score_retriever = AnswerScoreRetrieverForQuestions()\n",
    "        self.top_k_filter = TopKFilter()\n",
    "\n",
    "    def run(self, queries: List[Topic]) -> List[Tuple[Topic, Answer, float]]:\n",
    "        questions: List[Question] = self.data_reader.get_questions()\n",
    "        ranking: List[\n",
    "        Tuple[Topic, Union[Question, Answer], float]] = self.sbert(queries=queries, documents=questions)\n",
    "        ranking = self.answer_score_retriever(queries=queries, ranking=ranking)\n",
    "        ranking = self.top_k_filter(queries=queries, ranking=ranking)\n",
    "        return ranking\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-06 12:18:21.048262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at tbs17/MathBERT were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "tokenizing docs: 100%|██████████| 1020585/1020585 [04:10<00:00, 4068.30it/s]\n",
      "embedding docs:  11%|█         | 113436/1020585 [1:07:58<8:36:52, 29.25it/s] "
     ]
    }
   ],
   "source": [
    "from src.runner import Runner\n",
    "from datetime import datetime\n",
    "\n",
    "print(datetime.now())\n",
    "runner = Runner(MathBertPipeline, n=1, data_reader=data_reader, topic_reader=topic_reader)\n",
    "ranking = runner.run(\"../results/model_results/MathBert.tsv\")\n",
    "print(datetime.now())\n",
    "ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from arqmath_code.evaluation.task1 import arqmath_to_prime_task1\n",
    "from arqmath_code.evaluation.task1 import task1_get_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "qrel_dictionary = arqmath_to_prime_task1.read_qrel_to_dictionary(\"../arqmath_dataset/evaluation/Task 1/Qrel Files/qrel_task1_2022_official.tsv\")\n",
    "arqmath_to_prime_task1.convert_result_files_to_trec(submission_dir=\"../results/model_results/\", qrel_result_dic=qrel_dictionary, prim_dir=\"../results/ARQmath_prim/\", trec_dir=\"../results/ARQmath_trec/\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "number_topics = 78\n",
    "task1_get_results.get_result(trec_eval_tool=\"trec_eval\", qre_file_path=\"../arqmath_dataset/evaluation/Task 1/Qrel Files/qrel_task1_2022_official.tsv\", prim_result_dir=\"../results/ARQmath_prim/\", evaluation_result_file=\"../results/results_MathBert.tsv\", number_topics=number_topics)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}